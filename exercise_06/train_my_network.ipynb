{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Some lengthy setup.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from exercise_code.networks.layer import (\n",
    "    Sigmoid, \n",
    "    Relu, \n",
    "    LeakyRelu, \n",
    "    Tanh,\n",
    ")\n",
    "from exercise_code.data import (\n",
    "    DataLoader,\n",
    "    ImageFolderDataset,\n",
    "    RescaleTransform,\n",
    "    NormalizeTransform,\n",
    "    FlattenTransform,\n",
    "    ComposeTransform,\n",
    ")\n",
    "from exercise_code.data.image_folder_dataset import RandomHorizontalFlip\n",
    "from exercise_code.networks import (\n",
    "    ClassificationNet,\n",
    "    BCE,\n",
    "    CrossEntropyFromLogits\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = \"https://i2dl.vc.in.tum.de/static/data/cifar10.zip\"\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "cifar_root = os.path.join(i2dl_exercises_path, \"datasets\", \"cifar10\")\n",
    "\n",
    "# Use the Cifar10 mean and standard deviation computed in Exercise 3.\n",
    "cifar_mean = np.array([0.49191375, 0.48235852, 0.44673872])\n",
    "cifar_std  = np.array([0.24706447, 0.24346213, 0.26147554])\n",
    "\n",
    "# Define all the transforms we will apply on the images when \n",
    "# retrieving them.\n",
    "rescale_transform = RescaleTransform()\n",
    "normalize_transform = NormalizeTransform(\n",
    "    mean=cifar_mean,\n",
    "    std=cifar_std\n",
    ")\n",
    "flatten_transform = FlattenTransform()\n",
    "random_horizontal_flip = RandomHorizontalFlip()\n",
    "compose_transform = ComposeTransform([random_horizontal_flip, rescale_transform, \n",
    "                                      normalize_transform,\n",
    "                                      flatten_transform])\n",
    "\n",
    "# Create a train, validation and test dataset.\n",
    "datasets = {}\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    crt_dataset = ImageFolderDataset(\n",
    "        mode=mode,\n",
    "        root=cifar_root, \n",
    "        download_url=download_url,\n",
    "        transform=compose_transform,\n",
    "        split={'train': 0.6, 'val': 0.2, 'test': 0.2}\n",
    "    )\n",
    "    datasets[mode] = crt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer = 2\n",
    "epochs = 20\n",
    "reg = 0.1\n",
    "batch_size = 4\n",
    "\n",
    "# Create a dataloader for each split.\n",
    "dataloaders = {}\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    crt_dataloader = DataLoader(\n",
    "        dataset=datasets[mode],\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    dataloaders[mode] = crt_dataloader\n",
    "    \n",
    "num_samples = 500\n",
    "overfit_dataset = ImageFolderDataset(\n",
    "    mode='train',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=num_samples\n",
    ")\n",
    "dataloaders['train_small'] = DataLoader(\n",
    "    dataset=overfit_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "overfit_validset = ImageFolderDataset(\n",
    "    mode='train',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=200\n",
    ")\n",
    "dataloaders['val_small'] = DataLoader(\n",
    "    dataset=overfit_validset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 50) train loss: 2.302509; val loss: 2.302512\n",
      "(Epoch 2 / 50) train loss: 1.921325; val loss: 1.747843\n",
      "(Epoch 3 / 50) train loss: 1.666675; val loss: 1.641564\n",
      "(Epoch 4 / 50) train loss: 1.570900; val loss: 1.578219\n",
      "(Epoch 5 / 50) train loss: 1.511414; val loss: 1.545183\n",
      "(Epoch 6 / 50) train loss: 1.467319; val loss: 1.509647\n",
      "(Epoch 7 / 50) train loss: 1.428303; val loss: 1.493003\n",
      "(Epoch 8 / 50) train loss: 1.394063; val loss: 1.475999\n",
      "(Epoch 9 / 50) train loss: 1.365992; val loss: 1.447113\n",
      "(Epoch 10 / 50) train loss: 1.341182; val loss: 1.453729\n",
      "(Epoch 11 / 50) train loss: 1.314819; val loss: 1.434822\n",
      "(Epoch 12 / 50) train loss: 1.291106; val loss: 1.420077\n",
      "(Epoch 13 / 50) train loss: 1.269047; val loss: 1.413498\n",
      "(Epoch 14 / 50) train loss: 1.249591; val loss: 1.407223\n",
      "(Epoch 15 / 50) train loss: 1.228562; val loss: 1.405372\n",
      "(Epoch 16 / 50) train loss: 1.209651; val loss: 1.392756\n",
      "(Epoch 17 / 50) train loss: 1.194896; val loss: 1.390907\n",
      "(Epoch 18 / 50) train loss: 1.179487; val loss: 1.396912\n",
      "(Epoch 19 / 50) train loss: 1.160113; val loss: 1.379337\n",
      "(Epoch 20 / 50) train loss: 1.146330; val loss: 1.379412\n",
      "(Epoch 21 / 50) train loss: 1.129367; val loss: 1.384527\n",
      "(Epoch 22 / 50) train loss: 1.119355; val loss: 1.381118\n",
      "(Epoch 23 / 50) train loss: 1.104695; val loss: 1.391150\n",
      "(Epoch 24 / 50) train loss: 1.086626; val loss: 1.386087\n",
      "(Epoch 25 / 50) train loss: 1.072573; val loss: 1.371748\n",
      "(Epoch 26 / 50) train loss: 1.064307; val loss: 1.372356\n",
      "(Epoch 27 / 50) train loss: 1.049691; val loss: 1.394424\n",
      "(Epoch 28 / 50) train loss: 1.037256; val loss: 1.376316\n",
      "(Epoch 29 / 50) train loss: 1.025173; val loss: 1.375069\n",
      "(Epoch 30 / 50) train loss: 1.011701; val loss: 1.383540\n",
      "(Epoch 31 / 50) train loss: 1.002884; val loss: 1.381878\n",
      "(Epoch 32 / 50) train loss: 0.990587; val loss: 1.384840\n",
      "(Epoch 33 / 50) train loss: 0.979151; val loss: 1.391766\n",
      "(Epoch 34 / 50) train loss: 0.969088; val loss: 1.388126\n",
      "(Epoch 35 / 50) train loss: 0.957222; val loss: 1.403189\n",
      "(Epoch 36 / 50) train loss: 0.953287; val loss: 1.392981\n",
      "(Epoch 37 / 50) train loss: 0.935585; val loss: 1.395463\n",
      "(Epoch 38 / 50) train loss: 0.926852; val loss: 1.409131\n",
      "(Epoch 39 / 50) train loss: 0.913464; val loss: 1.407154\n",
      "(Epoch 40 / 50) train loss: 0.908082; val loss: 1.404843\n",
      "(Epoch 41 / 50) train loss: 0.898020; val loss: 1.411641\n",
      "(Epoch 42 / 50) train loss: 0.889364; val loss: 1.408200\n",
      "(Epoch 43 / 50) train loss: 0.876992; val loss: 1.419575\n",
      "(Epoch 44 / 50) train loss: 0.869423; val loss: 1.420597\n",
      "(Epoch 45 / 50) train loss: 0.860911; val loss: 1.426289\n",
      "(Epoch 46 / 50) train loss: 0.853586; val loss: 1.450246\n",
      "(Epoch 47 / 50) train loss: 0.842901; val loss: 1.439424\n",
      "(Epoch 48 / 50) train loss: 0.832886; val loss: 1.436651\n",
      "(Epoch 49 / 50) train loss: 0.826740; val loss: 1.446297\n",
      "(Epoch 50 / 50) train loss: 0.815054; val loss: 1.455849\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.solver import Solver\n",
    "from exercise_code.networks.optimizer import SGD, Adam\n",
    "from exercise_code.networks import MyOwnNetwork\n",
    "best_model = ClassificationNet(activation=LeakyRelu(), hidden_size=182, reg=2.0317705811992417e-06)\n",
    "loss = CrossEntropyFromLogits()\n",
    "solver = Solver(best_model, dataloaders['train'], dataloaders['val'], \n",
    "                learning_rate=0.00030599514043968363, loss_func=loss, activation=LeakyRelu, optimizer=Adam, patience=5)\n",
    "\n",
    "solver.train(epochs=50)\n",
    "best_model = solver.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 73.8147702991453%\n",
      "Validation Accuracy: 52.54407051282052%\n"
     ]
    }
   ],
   "source": [
    "labels, pred, acc = best_model.get_dataset_prediction(dataloaders['train'])\n",
    "print(\"Train Accuracy: {}%\".format(acc*100))\n",
    "labels, pred, acc = best_model.get_dataset_prediction(dataloaders['val'])\n",
    "print(\"Validation Accuracy: {}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 53.215144230769226%\n"
     ]
    }
   ],
   "source": [
    "labels, pred, acc = best_model.get_dataset_prediction(dataloaders['test'])\n",
    "print(\"Test Accuracy: {}%\".format(acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.tests import save_pickle\n",
    "save_pickle({\"cifar_fcn\": best_model}, \"cifar_fcn.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant folders: ['exercise_code', 'models']\n",
      "notebooks files: ['1.cifar10_classification.ipynb', 'train_my_network.ipynb']\n",
      "Adding folder exercise_code\n",
      "Adding folder models\n",
      "Adding notebook 1.cifar10_classification.ipynb\n",
      "Adding notebook train_my_network.ipynb\n",
      "Zipping successful! Zip is stored under: /mnt/c/Users/Yang/Google Drive/i2dl/i2dl_exercises/exercise_06/exercise06.zip\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.submit import submit_exercise\n",
    "\n",
    "submit_exercise('exercise06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
